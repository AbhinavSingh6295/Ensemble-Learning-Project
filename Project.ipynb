{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d83aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651fcd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/ruyroa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ruyroa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ruyroa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ruyroa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "108befaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aba7c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c51d21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf40253",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eae9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b84b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove non letters\n",
    "\n",
    "# Regular expression for removing all non-letter characters in the file.\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "def remove_non_letters(word):\n",
    "    return regex.sub(\"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974f4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word, stopwords):\n",
    "    a = [x for x in word.split(' ') if x not in stopwords]\n",
    "    return ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddb69baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    # Tokenize the sentence\n",
    "def lemmatizer(string, word_lemmatizer):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[word_lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98653bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the stopwords of an only letters string\n",
    "def preprocess_string(string, stopwords, word_lemmatizer):\n",
    "    return lemmatizer(remove_stopwords(remove_non_letters(string).lower(),\n",
    "                                       stopwords), word_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a14e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "WNL = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a501a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet_text'] = df.tweet_text.apply(lambda x : preprocess_string(x, stopwords_en, WNL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c240a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tok_tweet\"] = df.clean_tweet_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "343bc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"len_tweet\"] = df.tweet_text.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb2c85da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">len_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>173.542042</td>\n",
       "      <td>80.052851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <td>139.320060</td>\n",
       "      <td>76.774127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>136.422300</td>\n",
       "      <td>71.352681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_cyberbullying</th>\n",
       "      <td>83.107363</td>\n",
       "      <td>45.510016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_cyberbullying</th>\n",
       "      <td>85.713281</td>\n",
       "      <td>91.682049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>197.999000</td>\n",
       "      <td>71.941532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      len_tweet           \n",
       "                           mean        std\n",
       "cyberbullying_type                        \n",
       "age                  173.542042  80.052851\n",
       "ethnicity            139.320060  76.774127\n",
       "gender               136.422300  71.352681\n",
       "not_cyberbullying     83.107363  45.510016\n",
       "other_cyberbullying   85.713281  91.682049\n",
       "religion             197.999000  71.941532"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by = [\"cyberbullying_type\"]).agg({\"len_tweet\":[np.mean, np.std]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24977d0d",
   "metadata": {},
   "source": [
    "### Using Bag of Words vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "75fa1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_tweet_text\"],\n",
    "                                                    df[\"cyberbullying_type\"],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state = 0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5bf445cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "86f91894",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      " |         1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated:: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed23a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "07d33855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier().fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1604828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7937517472742521\n",
      "\n",
      "[[2292    5    7   53   21    4]\n",
      " [   6 2346    9   23   30   10]\n",
      " [   5   12 1988  185  214    6]\n",
      " [  36   11  139 1220  901   86]\n",
      " [  22   13  138  825 1274   15]\n",
      " [   1   10   18  105   41 2237]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred, y_test))\n",
    "print(\"\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "992898d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fa2ba47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=0),\n",
       "             param_grid={'max_depth': [10, 20, 75, 100],\n",
       "                         'min_samples_split': [5, 10, 15, 25]})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgrid = {\"max_depth\": [10, 20, 75, 100],\n",
    "      \"min_samples_split\": [5, 10, 15, 25]}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state = 0), param_grid=pgrid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "09e3cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 75, 'min_samples_split': 15}\n",
      "0.8122728543472183\n",
      "\n",
      "[[2300    3    4   47   27    1]\n",
      " [   6 2348   11   10   42    7]\n",
      " [   5   14 1946  129  312    4]\n",
      " [  41   14  117  884 1274   63]\n",
      " [  20   11   96  235 1919    6]\n",
      " [   2   10   13   76   86 2225]]\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "y_pred = grid_search.best_estimator_.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "print(\"\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "541c2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c28dd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bagg = BaggingClassifier(DecisionTreeClassifier(random_state = 0, max_depth = 75,\n",
    "                                                        min_samples_split = 15), \n",
    "                             max_samples=0.5, max_features=0.5, n_estimators=50)\n",
    "clf_bagg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_bagg = clf_bagg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "685ff1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8279284316466312\n",
      "\n",
      "[[2332    2    0   30   17    1]\n",
      " [   4 2376    4    8   28    4]\n",
      " [   4   11 1965  164  261    5]\n",
      " [  46   17   68 1050 1128   84]\n",
      " [  26   14   76  319 1825   27]\n",
      " [   0    8   11   36   59 2298]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred_bagg, y_test))\n",
    "print(\"\")\n",
    "print(confusion_matrix(y_test, y_pred_bagg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1c9bf2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=75,\n",
       "                                                                               min_samples_split=15,\n",
       "                                                                               random_state=0),\n",
       "                                         n_estimators=50),\n",
       "             param_grid={'max_features': [0.4, 0.5, 0.6],\n",
       "                         'max_samples': [0.4, 0.5, 0.6]})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgrid = {\"max_samples\": [0.4, 0.5, 0.6],\n",
    "      \"max_features\": [0.4, 0.5, 0.6]}\n",
    "grid_search_bagg = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(random_state = 0, \n",
    "                                                                         max_depth = 75,\n",
    "                                                                         min_samples_split = 15),\n",
    "                                                 n_estimators = 50), \n",
    "                                param_grid=pgrid, cv=5)\n",
    "\n",
    "grid_search_bagg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6bcee3d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 0.5, 'max_samples': 0.5}\n",
      "0.8270897400055913\n",
      "\n",
      "[[2326    0    2   36   18    0]\n",
      " [   4 2369    7    9   31    4]\n",
      " [   6    9 1944  174  270    7]\n",
      " [  39   16   60 1060 1132   86]\n",
      " [  19   10   70  329 1839   20]\n",
      " [   0    4   12   33   67 2296]]\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_bagg.best_params_)\n",
    "y_pred_bagg_gs = grid_search_bagg.best_estimator_.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_pred_bagg_gs, y_test))\n",
    "print(\"\")\n",
    "print(confusion_matrix(y_test, y_pred_bagg_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "edabdec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                age       0.97      0.98      0.97      2382\n",
      "          ethnicity       0.98      0.98      0.98      2424\n",
      "             gender       0.93      0.81      0.86      2410\n",
      "  not_cyberbullying       0.65      0.44      0.53      2393\n",
      "other_cyberbullying       0.55      0.80      0.65      2287\n",
      "           religion       0.95      0.95      0.95      2412\n",
      "\n",
      "           accuracy                           0.83     14308\n",
      "          macro avg       0.84      0.83      0.82     14308\n",
      "       weighted avg       0.84      0.83      0.83     14308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_bagg_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a81ded30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bw = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "rf_bw.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf_bw.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0ef5d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821917808219178\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b7f9f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "aa2949a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                age       0.97      0.98      0.97      2382\n",
      "          ethnicity       0.99      0.98      0.99      2424\n",
      "             gender       0.91      0.83      0.87      2410\n",
      "  not_cyberbullying       0.59      0.49      0.53      2393\n",
      "other_cyberbullying       0.54      0.68      0.60      2287\n",
      "           religion       0.95      0.96      0.96      2412\n",
      "\n",
      "           accuracy                           0.82     14308\n",
      "          macro avg       0.83      0.82      0.82     14308\n",
      "       weighted avg       0.83      0.82      0.82     14308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "6a0114c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(random_state = 0, \n",
    "                                                                max_depth = 5,\n",
    "                                                                min_samples_split = 2), \n",
    "                         n_estimators = 150, \n",
    "                         learning_rate = 0.1).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c1a6dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                age       0.99      0.94      0.96      2382\n",
      "          ethnicity       0.99      0.96      0.98      2424\n",
      "             gender       0.96      0.66      0.78      2410\n",
      "  not_cyberbullying       0.44      0.38      0.41      2393\n",
      "other_cyberbullying       0.47      0.85      0.61      2287\n",
      "           religion       0.98      0.73      0.84      2412\n",
      "\n",
      "           accuracy                           0.75     14308\n",
      "          macro avg       0.80      0.76      0.76     14308\n",
      "       weighted avg       0.81      0.75      0.76     14308\n",
      "\n",
      "0.7546128040257198\n"
     ]
    }
   ],
   "source": [
    "y_pred = ada.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4bf6f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e06f40f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=150)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1) \n",
    "gbc.fit(X_train_tfidf, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d0d0851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                age       0.98      0.97      0.97      2382\n",
      "          ethnicity       0.99      0.97      0.98      2424\n",
      "             gender       0.92      0.83      0.87      2410\n",
      "  not_cyberbullying       0.70      0.42      0.53      2393\n",
      "other_cyberbullying       0.55      0.87      0.67      2287\n",
      "           religion       0.97      0.92      0.94      2412\n",
      "\n",
      "           accuracy                           0.83     14308\n",
      "          macro avg       0.85      0.83      0.83     14308\n",
      "       weighted avg       0.85      0.83      0.83     14308\n",
      "\n",
      "0.8313530891808778\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbc.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aa12921c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        47692\n",
       "unique           6\n",
       "top       religion\n",
       "freq          7998\n",
       "Name: cyberbullying_type, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cyberbullying_type.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349525e",
   "metadata": {},
   "source": [
    "### Using Word2Vec for the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "740b1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok, X_test_tok, y_train_w2v, y_test_w2v = train_test_split(df[\"clean_tok_tweet\"],\n",
    "                                                    df[\"cyberbullying_type\"],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state = 0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1f547567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "    def transform(self, X):\n",
    "            return np.array([\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                        or [np.zeros(self.dim)], axis=0)\n",
    "                for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1b9b56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "41571eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None)\n",
      " |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |      \n",
      " |      Once you're finished training a model (=no more updates, only querying)\n",
      " |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      " |      to reduce memory.\n",
      " |      \n",
      " |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |      \n",
      " |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, rethrow=False, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d331966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(df['clean_tok_tweet'],min_count=3, vector_size = 500, window = 3)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors)) \n",
    "modelw = MeanEmbeddingVectorizer(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "03fdd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = modelw.transform(X_train_tok)\n",
    "X_test_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6fe0794e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33384, 1000)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c9b8a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = 15, \n",
    "                             min_samples_split = 2,\n",
    "                            min_samples_leaf = 2).fit(X_train_w2v, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f00ce1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_w2v = clf.predict(X_test_w2v)\n",
    "\n",
    "print(classification_report(y_test_w2v, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "794d6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7113502935420744\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred_w2v, y_test_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8c4639e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 15, 'min_samples_leaf': 7, 'min_samples_split': 10}\n",
      "\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                age       0.87      0.90      0.88      2382\n",
      "          ethnicity       0.83      0.82      0.82      2424\n",
      "             gender       0.78      0.69      0.73      2410\n",
      "  not_cyberbullying       0.47      0.41      0.44      2393\n",
      "other_cyberbullying       0.42      0.52      0.46      2287\n",
      "           religion       0.77      0.76      0.77      2412\n",
      "\n",
      "           accuracy                           0.69     14308\n",
      "          macro avg       0.69      0.68      0.69     14308\n",
      "       weighted avg       0.69      0.69      0.69     14308\n",
      "\n",
      "\n",
      "0.6854207436399217\n",
      "\n",
      "[[2138   28   14   72   93   37]\n",
      " [  40 1984   68   71  176   85]\n",
      " [  47   68 1653  234  314   94]\n",
      " [  97   77  171  992  877  179]\n",
      " [  81  129  152  580 1198  147]\n",
      " [  48  111   51  144  216 1842]]\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_w2v.best_params_)\n",
    "print(\"\")\n",
    "y_pred_w2v = grid_search_w2v.best_estimator_.predict(X_test_w2v)\n",
    "print(\"\")\n",
    "print(classification_report(y_test_w2v, y_pred_w2v))\n",
    "print(\"\")\n",
    "print(accuracy_score(y_test_w2v, y_pred_w2v))\n",
    "print(\"\")\n",
    "print(confusion_matrix(y_test_w2v, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ebf08920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.5, n_estimators=20)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_w2v = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5) \n",
    "gbc_w2v.fit(X_train_w2v, y_train_w2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a4d5b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                age       0.94      0.96      0.95      2382\n",
      "          ethnicity       0.90      0.90      0.90      2424\n",
      "             gender       0.86      0.72      0.79      2410\n",
      "  not_cyberbullying       0.56      0.48      0.52      2393\n",
      "other_cyberbullying       0.49      0.61      0.54      2287\n",
      "           religion       0.86      0.89      0.88      2412\n",
      "\n",
      "           accuracy                           0.76     14308\n",
      "          macro avg       0.77      0.76      0.76     14308\n",
      "       weighted avg       0.77      0.76      0.76     14308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_w2v = gbc_w2v.predict(X_test_w2v)\n",
    "\n",
    "print(classification_report(y_test_w2v, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13407440",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_registered = []\n",
    "words_count = []\n",
    "words_index = dict()\n",
    "i = 0\n",
    "for line in df.iloc[:,0]:\n",
    "    word_list = preprocess_string(line, stopwords_en)\n",
    "    for word in word_list:\n",
    "        if word not in words_registered:\n",
    "            words_registered.append(word)\n",
    "            words_count.append([word, 1])\n",
    "            words_index[word] = i\n",
    "            i += 1\n",
    "        else:\n",
    "            words_count[words_index[word]][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52e0175a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 17222],\n",
       " ['school', 8725],\n",
       " ['like', 5866],\n",
       " ['fuck', 5799],\n",
       " ['dumb', 5336],\n",
       " ['high', 5098],\n",
       " ['people', 4807],\n",
       " ['bullied', 4666],\n",
       " ['im', 4488],\n",
       " ['dont', 4407],\n",
       " ['nigger', 4318],\n",
       " ['rape', 4247],\n",
       " ['rt', 4230],\n",
       " ['u', 4106],\n",
       " ['gay', 4008],\n",
       " ['bully', 3613],\n",
       " ['jokes', 3470],\n",
       " ['one', 3402],\n",
       " ['get', 2846],\n",
       " ['girls', 2834],\n",
       " ['ass', 2794],\n",
       " ['girl', 2710],\n",
       " ['black', 2701],\n",
       " ['mkr', 2659],\n",
       " ['amp', 2608],\n",
       " ['muslims', 2424],\n",
       " ['muslim', 2345],\n",
       " ['know', 2340],\n",
       " ['white', 2191],\n",
       " ['think', 2129],\n",
       " ['would', 2054],\n",
       " ['joke', 2041],\n",
       " ['bitch', 2026],\n",
       " ['idiot', 1993],\n",
       " ['right', 1838],\n",
       " ['say', 1743],\n",
       " ['got', 1731],\n",
       " ['fucking', 1679],\n",
       " ['call', 1676],\n",
       " ['even', 1657],\n",
       " ['bullies', 1652],\n",
       " ['go', 1640],\n",
       " ['women', 1623],\n",
       " ['cant', 1616],\n",
       " ['make', 1612],\n",
       " ['youre', 1567],\n",
       " ['christian', 1536],\n",
       " ['never', 1528],\n",
       " ['see', 1525],\n",
       " ['ur', 1504],\n",
       " ['called', 1499],\n",
       " ['shit', 1496],\n",
       " ['time', 1483],\n",
       " ['idiots', 1470],\n",
       " ['woman', 1460],\n",
       " ['still', 1449],\n",
       " ['really', 1442],\n",
       " ['thats', 1428],\n",
       " ['want', 1413],\n",
       " ['first', 1334],\n",
       " ['hate', 1327],\n",
       " ['us', 1310],\n",
       " ['said', 1305],\n",
       " ['niggers', 1296],\n",
       " ['islamic', 1280],\n",
       " ['islam', 1275],\n",
       " ['support', 1267],\n",
       " ['obama', 1257],\n",
       " ['good', 1243],\n",
       " ['bullying', 1235],\n",
       " ['also', 1213],\n",
       " ['anything', 1195],\n",
       " ['man', 1152],\n",
       " ['terrorism', 1121],\n",
       " ['didnt', 1088],\n",
       " ['back', 1084],\n",
       " ['need', 1083],\n",
       " ['radical', 1082],\n",
       " ['love', 1075],\n",
       " ['lol', 1066],\n",
       " ['way', 1063],\n",
       " ['sexist', 1043],\n",
       " ['stop', 1043],\n",
       " ['going', 1027],\n",
       " ['made', 1023],\n",
       " ['men', 1013],\n",
       " ['racist', 999],\n",
       " ['bad', 973],\n",
       " ['doesnt', 956],\n",
       " ['much', 950],\n",
       " ['every', 944],\n",
       " ['someone', 940],\n",
       " ['look', 931],\n",
       " ['funny', 931],\n",
       " ['tayyoung', 925],\n",
       " ['female', 905],\n",
       " ['mean', 903],\n",
       " ['racism', 892],\n",
       " ['many', 890],\n",
       " ['well', 879],\n",
       " ['past', 879],\n",
       " ['always', 863],\n",
       " ['years', 859],\n",
       " ['ive', 852],\n",
       " ['middle', 851],\n",
       " ['saying', 847],\n",
       " ['life', 847],\n",
       " ['world', 844],\n",
       " ['thing', 832],\n",
       " ['day', 822],\n",
       " ['isis', 804],\n",
       " ['hes', 791],\n",
       " ['oh', 784],\n",
       " ['ever', 783],\n",
       " ['person', 782],\n",
       " ['yall', 779],\n",
       " ['take', 775],\n",
       " ['friends', 770],\n",
       " ['terrorist', 769],\n",
       " ['name', 767],\n",
       " ['kids', 754],\n",
       " ['tell', 745],\n",
       " ['making', 733],\n",
       " ['stupid', 727],\n",
       " ['god', 714],\n",
       " ['r', 710],\n",
       " ['isnt', 700],\n",
       " ['things', 695],\n",
       " ['better', 684],\n",
       " ['used', 683],\n",
       " ['terrorists', 678],\n",
       " ['quran', 675],\n",
       " ['twitter', 674],\n",
       " ['getting', 673],\n",
       " ['feel', 669],\n",
       " ['everyone', 669],\n",
       " ['makes', 665],\n",
       " ['use', 664],\n",
       " ['yes', 659],\n",
       " ['nothing', 658],\n",
       " ['something', 655],\n",
       " ['word', 644],\n",
       " ['could', 638],\n",
       " ['disgusting', 633],\n",
       " ['trump', 627],\n",
       " ['christians', 620],\n",
       " ['let', 620],\n",
       " ['talk', 618],\n",
       " ['country', 615],\n",
       " ['guy', 615],\n",
       " ['real', 613],\n",
       " ['give', 606],\n",
       " ['trying', 594],\n",
       " ['guys', 593],\n",
       " ['religion', 591],\n",
       " ['believe', 585],\n",
       " ['theyre', 584],\n",
       " ['lot', 583],\n",
       " ['wrong', 583],\n",
       " ['ppl', 580],\n",
       " ['jihad', 580],\n",
       " ['humanity', 580],\n",
       " ['old', 578],\n",
       " ['come', 568],\n",
       " ['calling', 567],\n",
       " ['kat', 558],\n",
       " ['year', 552],\n",
       " ['actually', 549],\n",
       " ['anyone', 549],\n",
       " ['new', 548],\n",
       " ['negro', 548],\n",
       " ['talking', 544],\n",
       " ['shes', 544],\n",
       " ['ok', 542],\n",
       " ['another', 539],\n",
       " ['yeah', 538],\n",
       " ['find', 535],\n",
       " ['read', 533],\n",
       " ['please', 532],\n",
       " ['show', 531],\n",
       " ['keep', 531],\n",
       " ['india', 525],\n",
       " ['told', 524],\n",
       " ['mad', 517],\n",
       " ['try', 514],\n",
       " ['n', 511],\n",
       " ['today', 508],\n",
       " ['ugly', 506],\n",
       " ['pretty', 505],\n",
       " ['little', 501],\n",
       " ['ill', 498],\n",
       " ['around', 496],\n",
       " ['sure', 496],\n",
       " ['colored', 490],\n",
       " ['went', 485],\n",
       " ['bitches', 483],\n",
       " ['work', 482],\n",
       " ['kill', 478],\n",
       " ['hell', 477],\n",
       " ['arent', 475],\n",
       " ['thought', 474],\n",
       " ['says', 474],\n",
       " ['remember', 471],\n",
       " ['done', 468],\n",
       " ['since', 461],\n",
       " ['gonna', 459],\n",
       " ['hope', 455],\n",
       " ['face', 455],\n",
       " ['put', 452],\n",
       " ['friend', 450],\n",
       " ['bc', 450],\n",
       " ['cause', 448],\n",
       " ['sorry', 447],\n",
       " ['theres', 445],\n",
       " ['big', 442],\n",
       " ['two', 440],\n",
       " ['care', 438],\n",
       " ['schools', 435],\n",
       " ['help', 433],\n",
       " ['literally', 433],\n",
       " ['best', 432],\n",
       " ['nigga', 432],\n",
       " ['must', 429],\n",
       " ['point', 428],\n",
       " ['whole', 422],\n",
       " ['wont', 419],\n",
       " ['feminazi', 418],\n",
       " ['kid', 416],\n",
       " ['last', 406],\n",
       " ['shut', 406],\n",
       " ['lmao', 396],\n",
       " ['wasnt', 395],\n",
       " ['etc', 394],\n",
       " ['problem', 393],\n",
       " ['rights', 392],\n",
       " ['understand', 391],\n",
       " ['boys', 388],\n",
       " ['group', 387],\n",
       " ['kind', 386],\n",
       " ['class', 382],\n",
       " ['fact', 380],\n",
       " ['tweet', 379],\n",
       " ['killed', 376],\n",
       " ['hard', 375],\n",
       " ['left', 375],\n",
       " ['video', 374],\n",
       " ['using', 374],\n",
       " ['seen', 368],\n",
       " ['enough', 367],\n",
       " ['okay', 366],\n",
       " ['everything', 365],\n",
       " ['looks', 365],\n",
       " ['watch', 364],\n",
       " ['fight', 363],\n",
       " ['children', 362],\n",
       " ['gays', 357],\n",
       " ['great', 356],\n",
       " ['stand', 355],\n",
       " ['though', 353],\n",
       " ['ask', 353],\n",
       " ['yet', 351],\n",
       " ['fun', 351],\n",
       " ['away', 350],\n",
       " ['id', 350],\n",
       " ['p', 349],\n",
       " ['next', 349],\n",
       " ['aint', 349],\n",
       " ['hindus', 349],\n",
       " ['fat', 348],\n",
       " ['hindu', 348],\n",
       " ['matter', 346],\n",
       " ['home', 344],\n",
       " ['money', 343],\n",
       " ['reason', 343],\n",
       " ['boy', 342],\n",
       " ['others', 340],\n",
       " ['damn', 340],\n",
       " ['wanna', 339],\n",
       " ['ones', 336],\n",
       " ['head', 336],\n",
       " ['different', 334],\n",
       " ['part', 331],\n",
       " ['already', 330],\n",
       " ['without', 330],\n",
       " ['community', 330],\n",
       " ['probably', 329],\n",
       " ['live', 329],\n",
       " ['hey', 326],\n",
       " ['race', 326],\n",
       " ['maybe', 325],\n",
       " ['long', 325],\n",
       " ['family', 324],\n",
       " ['start', 323],\n",
       " ['times', 322],\n",
       " ['dude', 317],\n",
       " ['change', 317],\n",
       " ['yo', 317],\n",
       " ['far', 313],\n",
       " ['else', 313],\n",
       " ['females', 313],\n",
       " ['end', 312],\n",
       " ['nice', 311],\n",
       " ['prison', 308],\n",
       " ['saw', 307],\n",
       " ['wouldnt', 306],\n",
       " ['started', 306],\n",
       " ['killing', 306],\n",
       " ['means', 304],\n",
       " ['guess', 303],\n",
       " ['true', 303],\n",
       " ['words', 302],\n",
       " ['lives', 302],\n",
       " ['watching', 301],\n",
       " ['act', 299],\n",
       " ['police', 298],\n",
       " ['grade', 296],\n",
       " ['human', 295],\n",
       " ['america', 294],\n",
       " ['looking', 293],\n",
       " ['male', 293],\n",
       " ['mom', 292],\n",
       " ['follow', 291],\n",
       " ['house', 291],\n",
       " ['ago', 291],\n",
       " ['child', 290],\n",
       " ['whats', 289],\n",
       " ['post', 289],\n",
       " ['hair', 289],\n",
       " ['sex', 289],\n",
       " ['pakistan', 288],\n",
       " ['president', 287],\n",
       " ['weird', 286],\n",
       " ['lets', 285],\n",
       " ['media', 283],\n",
       " ['cool', 280],\n",
       " ['stuff', 280],\n",
       " ['w', 279],\n",
       " ['american', 279],\n",
       " ['jews', 278],\n",
       " ['gets', 277],\n",
       " ['abuse', 277],\n",
       " ['play', 275],\n",
       " ['speak', 274],\n",
       " ['happy', 273],\n",
       " ['ignorant', 272],\n",
       " ['fucks', 272],\n",
       " ['might', 269],\n",
       " ['place', 268],\n",
       " ['tweets', 268],\n",
       " ['sick', 266],\n",
       " ['war', 266],\n",
       " ['days', 266],\n",
       " ['thanks', 265],\n",
       " ['story', 265],\n",
       " ['fake', 261],\n",
       " ['idea', 260],\n",
       " ['beat', 260],\n",
       " ['either', 260],\n",
       " ['ya', 259],\n",
       " ['history', 259],\n",
       " ['happened', 258],\n",
       " ['needs', 257],\n",
       " ['free', 257],\n",
       " ['seeing', 257],\n",
       " ['death', 256],\n",
       " ['may', 256],\n",
       " ['b', 254],\n",
       " ['found', 254],\n",
       " ['game', 254],\n",
       " ['th', 254],\n",
       " ['came', 250],\n",
       " ['job', 249],\n",
       " ['goes', 249],\n",
       " ['become', 249],\n",
       " ['lt', 248],\n",
       " ['wanted', 247],\n",
       " ['date', 246],\n",
       " ['haha', 246],\n",
       " ['self', 244],\n",
       " ['law', 244],\n",
       " ['social', 243],\n",
       " ['mind', 243],\n",
       " ['news', 242],\n",
       " ['andre', 242],\n",
       " ['hear', 241],\n",
       " ['wow', 240],\n",
       " ['public', 239],\n",
       " ['seriously', 239],\n",
       " ['thinking', 238],\n",
       " ['omg', 238],\n",
       " ['respect', 238],\n",
       " ['full', 238],\n",
       " ['calls', 238],\n",
       " ['wait', 236],\n",
       " ['party', 235],\n",
       " ['thank', 235],\n",
       " ['freebsdgirl', 234],\n",
       " ['less', 234],\n",
       " ['parents', 233],\n",
       " ['wish', 232],\n",
       " ['worst', 230],\n",
       " ['heard', 230],\n",
       " ['leave', 230],\n",
       " ['dick', 230],\n",
       " ['religious', 229],\n",
       " ['power', 227],\n",
       " ['coming', 227],\n",
       " ['straight', 227],\n",
       " ['least', 226],\n",
       " ['wants', 226],\n",
       " ['agree', 225],\n",
       " ['instead', 223],\n",
       " ['cuz', 223],\n",
       " ['telling', 223],\n",
       " ['sad', 223],\n",
       " ['countries', 223],\n",
       " ['knew', 222],\n",
       " ['homophobic', 222],\n",
       " ['stay', 221],\n",
       " ['culture', 220],\n",
       " ['vote', 220],\n",
       " ['state', 218],\n",
       " ['jesus', 218],\n",
       " ['night', 217],\n",
       " ['non', 217],\n",
       " ['tried', 215],\n",
       " ['elementary', 215],\n",
       " ['indian', 215],\n",
       " ['de', 212],\n",
       " ['murder', 212],\n",
       " ['living', 212],\n",
       " ['comes', 212],\n",
       " ['run', 209],\n",
       " ['shouldnt', 209],\n",
       " ['learn', 208],\n",
       " ['later', 208],\n",
       " ['book', 208],\n",
       " ['smh', 207],\n",
       " ['gave', 207],\n",
       " ['college', 206],\n",
       " ['havent', 206],\n",
       " ['taking', 206],\n",
       " ['niggas', 206],\n",
       " ['insult', 206],\n",
       " ['skin', 204],\n",
       " ['exactly', 204],\n",
       " ['behind', 203],\n",
       " ['single', 202],\n",
       " ['attack', 202],\n",
       " ['miley', 202],\n",
       " ['soon', 201],\n",
       " ['tho', 201],\n",
       " ['cannot', 201],\n",
       " ['together', 201],\n",
       " ['wtf', 200],\n",
       " ['shame', 200],\n",
       " ['evil', 200],\n",
       " ['thinks', 199],\n",
       " ['bit', 199],\n",
       " ['whatever', 198],\n",
       " ['anti', 197],\n",
       " ['prophet', 197],\n",
       " ['bunch', 196],\n",
       " ['via', 195],\n",
       " ['entire', 195],\n",
       " ['popular', 194],\n",
       " ['cyrus', 194],\n",
       " ['violence', 193],\n",
       " ['facebook', 193],\n",
       " ['forget', 193],\n",
       " ['die', 193],\n",
       " ['victim', 191],\n",
       " ['bro', 191],\n",
       " ['mohammed', 191],\n",
       " ['couldnt', 191],\n",
       " ['poor', 191],\n",
       " ['x', 190],\n",
       " ['young', 190],\n",
       " ['lost', 189],\n",
       " ['hit', 189],\n",
       " ['truth', 189],\n",
       " ['asked', 188],\n",
       " ['wearing', 187],\n",
       " ['type', 187],\n",
       " ['listen', 186],\n",
       " ['took', 186],\n",
       " ['crazy', 185],\n",
       " ['knows', 185],\n",
       " ['dog', 185],\n",
       " ['almost', 185],\n",
       " ['seems', 185],\n",
       " ['sound', 185],\n",
       " ['sense', 185],\n",
       " ['dead', 185],\n",
       " ['groups', 185],\n",
       " ['victims', 184],\n",
       " ['laugh', 184],\n",
       " ['e', 182],\n",
       " ['trash', 182],\n",
       " ['mother', 182],\n",
       " ['pay', 181],\n",
       " ['question', 181],\n",
       " ['open', 180],\n",
       " ['hot', 180],\n",
       " ['pick', 179],\n",
       " ['son', 179],\n",
       " ['week', 179],\n",
       " ['half', 179],\n",
       " ['gamergate', 178],\n",
       " ['definitely', 177],\n",
       " ['sexual', 177],\n",
       " ['que', 176],\n",
       " ['baby', 176],\n",
       " ['team', 175],\n",
       " ['able', 175],\n",
       " ['hatred', 175],\n",
       " ['rest', 174],\n",
       " ['harassment', 174],\n",
       " ['happen', 173],\n",
       " ['working', 173],\n",
       " ['allah', 173],\n",
       " ['imagine', 173],\n",
       " ['coon', 173],\n",
       " ['fuckin', 172],\n",
       " ['worse', 172],\n",
       " ['comments', 172],\n",
       " ['abt', 172],\n",
       " ['rather', 171],\n",
       " ['playing', 171],\n",
       " ['case', 171],\n",
       " ['asking', 171],\n",
       " ['yard', 171],\n",
       " ['hoe', 169],\n",
       " ['sometimes', 169],\n",
       " ['side', 169],\n",
       " ['allowed', 169],\n",
       " ['especially', 169],\n",
       " ['china', 169],\n",
       " ['wonder', 168],\n",
       " ['giving', 168],\n",
       " ['claim', 167],\n",
       " ['term', 167],\n",
       " ['trans', 167],\n",
       " ['account', 166],\n",
       " ['lying', 166],\n",
       " ['americans', 166],\n",
       " ['issue', 166],\n",
       " ['teacher', 165],\n",
       " ['hurt', 165],\n",
       " ['slavery', 165],\n",
       " ['notsexist', 165],\n",
       " ['majority', 165],\n",
       " ['gone', 164],\n",
       " ['honestly', 164],\n",
       " ['shows', 164],\n",
       " ['lie', 164],\n",
       " ['color', 164],\n",
       " ['alone', 164],\n",
       " ['clearly', 163],\n",
       " ['voice', 163],\n",
       " ['cunt', 163],\n",
       " ['innocent', 163],\n",
       " ['line', 162],\n",
       " ['movie', 162],\n",
       " ['grow', 162],\n",
       " ['maxblumenthal', 161],\n",
       " ['freedom', 161],\n",
       " ['blame', 161],\n",
       " ['peace', 161],\n",
       " ['nobody', 160],\n",
       " ['supporting', 160],\n",
       " ['teachers', 160],\n",
       " ['suck', 160],\n",
       " ['raped', 160],\n",
       " ['israel', 160],\n",
       " ['bullshit', 159],\n",
       " ['example', 159],\n",
       " ['pussy', 158],\n",
       " ['totally', 158],\n",
       " ['blacks', 158],\n",
       " ['daughter', 157],\n",
       " ['fucked', 157],\n",
       " ['deal', 157],\n",
       " ['online', 157],\n",
       " ['fighting', 157],\n",
       " ['fine', 157],\n",
       " ['bring', 156],\n",
       " ['issues', 156],\n",
       " ['suicide', 155],\n",
       " ['mental', 155],\n",
       " ['difference', 154],\n",
       " ['wash', 154],\n",
       " ['turn', 153],\n",
       " ['dad', 153],\n",
       " ['seem', 152],\n",
       " ['lil', 151],\n",
       " ['super', 151],\n",
       " ['comment', 150],\n",
       " ['biggest', 150],\n",
       " ['wear', 150],\n",
       " ['anymore', 149],\n",
       " ['piece', 149],\n",
       " ['became', 149],\n",
       " ['basically', 148],\n",
       " ['miss', 148],\n",
       " ['opinion', 148],\n",
       " ['whos', 148],\n",
       " ['liked', 148],\n",
       " ['happens', 148],\n",
       " ['food', 147],\n",
       " ['gotta', 147],\n",
       " ['tonight', 147],\n",
       " ['small', 147],\n",
       " ['government', 147],\n",
       " ['towards', 146],\n",
       " ['acting', 146],\n",
       " ['front', 146],\n",
       " ['names', 145],\n",
       " ['youll', 145],\n",
       " ['experience', 145],\n",
       " ['win', 145],\n",
       " ['folks', 145],\n",
       " ['bet', 145],\n",
       " ['brother', 145],\n",
       " ['nation', 144],\n",
       " ['given', 144],\n",
       " ['fear', 144],\n",
       " ['blm', 144],\n",
       " ['tall', 144],\n",
       " ['safe', 143],\n",
       " ['course', 143],\n",
       " ['second', 143],\n",
       " ['heart', 142],\n",
       " ['deserve', 142],\n",
       " ['football', 142],\n",
       " ['defend', 142],\n",
       " ['low', 141],\n",
       " ['annoying', 141],\n",
       " ['sounds', 141],\n",
       " ['save', 140],\n",
       " ['ha', 140],\n",
       " ['youve', 140],\n",
       " ['asshole', 140],\n",
       " ['hated', 140],\n",
       " ['tv', 139],\n",
       " ['attention', 139],\n",
       " ['proud', 139],\n",
       " ['takes', 138],\n",
       " ['move', 138],\n",
       " ['cut', 138],\n",
       " ['eat', 137],\n",
       " ['amazing', 137],\n",
       " ['offensive', 137],\n",
       " ['due', 136],\n",
       " ['horrible', 136],\n",
       " ['round', 136],\n",
       " ['top', 136],\n",
       " ['serious', 136],\n",
       " ['age', 136],\n",
       " ['choose', 135],\n",
       " ['clear', 135],\n",
       " ['mouth', 135],\n",
       " ['speech', 135],\n",
       " ['outside', 135],\n",
       " ['teach', 135],\n",
       " ['crime', 135],\n",
       " ['cute', 134],\n",
       " ['reading', 134],\n",
       " ['feminists', 134],\n",
       " ['sports', 134],\n",
       " ['idk', 134],\n",
       " ['em', 133],\n",
       " ['st', 133],\n",
       " ['da', 133],\n",
       " ['scared', 133],\n",
       " ['feminism', 133],\n",
       " ['political', 133],\n",
       " ['check', 132],\n",
       " ['clothes', 132],\n",
       " ['choice', 132],\n",
       " ['feminist', 132],\n",
       " ['nd', 132],\n",
       " ['kashmir', 132],\n",
       " ['mykitchenrules', 131],\n",
       " ['mostly', 130],\n",
       " ['unless', 130],\n",
       " ['realize', 130],\n",
       " ['werent', 130],\n",
       " ['set', 129],\n",
       " ['completely', 129],\n",
       " ['months', 129],\n",
       " ['laughing', 129],\n",
       " ['send', 129],\n",
       " ['body', 129],\n",
       " ['african', 129],\n",
       " ['students', 128],\n",
       " ['wadhwa', 128],\n",
       " ['speaking', 128],\n",
       " ['fans', 128],\n",
       " ['except', 128],\n",
       " ['sent', 127],\n",
       " ['turned', 127],\n",
       " ['positive', 127],\n",
       " ['blameonenotall', 127],\n",
       " ['lies', 126],\n",
       " ['jewish', 126],\n",
       " ['accept', 126],\n",
       " ['kinda', 126],\n",
       " ['facts', 126],\n",
       " ['angry', 125],\n",
       " ['everybody', 125],\n",
       " ['slave', 125],\n",
       " ['followers', 125],\n",
       " ['taken', 125],\n",
       " ['looked', 125],\n",
       " ['response', 124],\n",
       " ['absolutely', 124],\n",
       " ['health', 124],\n",
       " ['holy', 124],\n",
       " ['attacked', 124],\n",
       " ['incident', 124],\n",
       " ['se', 123],\n",
       " ['list', 123],\n",
       " ['church', 123],\n",
       " ['wife', 123],\n",
       " ['finally', 123],\n",
       " ['born', 123],\n",
       " ['actual', 123],\n",
       " ['music', 123],\n",
       " ['problems', 123],\n",
       " ['exist', 123],\n",
       " ['hand', 122],\n",
       " ['feeling', 122],\n",
       " ['k', 122],\n",
       " ['bout', 122],\n",
       " ['swear', 122],\n",
       " ['expect', 122],\n",
       " ['answer', 121],\n",
       " ['eyes', 121],\n",
       " ['sister', 121],\n",
       " ['whites', 121],\n",
       " ['junior', 121],\n",
       " ['glad', 120],\n",
       " ['mt', 120],\n",
       " ['none', 120],\n",
       " ['na', 120],\n",
       " ['strong', 120],\n",
       " ['mine', 120],\n",
       " ['smart', 120],\n",
       " ['dark', 120],\n",
       " ['obviously', 119],\n",
       " ['explain', 119],\n",
       " ['based', 119],\n",
       " ['following', 119],\n",
       " ['internet', 118],\n",
       " ['known', 118],\n",
       " ['awful', 118],\n",
       " ['excuse', 118],\n",
       " ['phone', 118],\n",
       " ['spread', 118],\n",
       " ['important', 118],\n",
       " ['land', 118],\n",
       " ['father', 118],\n",
       " ['red', 117],\n",
       " ['brown', 117],\n",
       " ['likes', 117],\n",
       " ['tired', 117],\n",
       " ['apparently', 116],\n",
       " ['states', 116],\n",
       " ['al', 116],\n",
       " ['liking', 116],\n",
       " ['sit', 116],\n",
       " ['daesh', 115],\n",
       " ['bloody', 115],\n",
       " ['close', 115],\n",
       " ['army', 115],\n",
       " ['gives', 115],\n",
       " ['level', 114],\n",
       " ['propaganda', 114],\n",
       " ['terrible', 114],\n",
       " ['met', 114],\n",
       " ['youtube', 114],\n",
       " ['general', 114],\n",
       " ['running', 114],\n",
       " ['society', 114],\n",
       " ['throughout', 114],\n",
       " ['spacekatgal', 113],\n",
       " ['short', 113],\n",
       " ['tech', 113],\n",
       " ['often', 113],\n",
       " ['number', 113],\n",
       " ['quite', 113],\n",
       " ['felt', 113],\n",
       " ['block', 112],\n",
       " ['cry', 112],\n",
       " ['hahaha', 112],\n",
       " ['write', 112],\n",
       " ['three', 112],\n",
       " ['report', 112],\n",
       " ['slaves', 112],\n",
       " ['games', 112],\n",
       " ['character', 112],\n",
       " ['tf', 112],\n",
       " ['happening', 111],\n",
       " ['cook', 111],\n",
       " ['early', 111],\n",
       " ['special', 111],\n",
       " ['crap', 111],\n",
       " ['dudes', 111],\n",
       " ['womens', 111],\n",
       " ['usually', 111],\n",
       " ['reasons', 111],\n",
       " ['offended', 111],\n",
       " ['feels', 110],\n",
       " ['student', 110],\n",
       " ['personal', 110],\n",
       " ['murdered', 110],\n",
       " ['million', 110],\n",
       " ['married', 110],\n",
       " ['slurs', 110],\n",
       " ['nasty', 109],\n",
       " ['system', 109],\n",
       " ['continue', 109],\n",
       " ['attacks', 109],\n",
       " ['easy', 109],\n",
       " ['business', 109],\n",
       " ['lesbian', 109],\n",
       " ['throw', 109],\n",
       " ['crying', 108],\n",
       " ['sir', 108],\n",
       " ['abortion', 108],\n",
       " ['cops', 108],\n",
       " ['af', 107],\n",
       " ['buy', 107],\n",
       " ['uses', 107],\n",
       " ['works', 107],\n",
       " ['liberal', 107],\n",
       " ['random', 107],\n",
       " ['changed', 107],\n",
       " ['club', 107],\n",
       " ['hold', 107],\n",
       " ['room', 107],\n",
       " ['false', 107],\n",
       " ['ban', 107],\n",
       " ['ready', 106],\n",
       " ['eu', 106],\n",
       " ['posted', 106],\n",
       " ['stopped', 106],\n",
       " ['supposed', 106],\n",
       " ['shot', 106],\n",
       " ['certain', 105],\n",
       " ['picture', 105],\n",
       " ['force', 105],\n",
       " ['equal', 105],\n",
       " ['gender', 105],\n",
       " ['reality', 105],\n",
       " ['mr', 105],\n",
       " ['morning', 104],\n",
       " ['defending', 104],\n",
       " ['sexism', 104],\n",
       " ['chriswarcraft', 104],\n",
       " ['needed', 104],\n",
       " ['shitty', 103],\n",
       " ['faith', 103],\n",
       " ['proof', 103],\n",
       " ['article', 103],\n",
       " ['rss', 103],\n",
       " ['violent', 103],\n",
       " ['education', 102],\n",
       " ['tells', 102],\n",
       " ['instant', 102],\n",
       " ['thequinnspiracy', 102],\n",
       " ['fan', 102],\n",
       " ['behavior', 102],\n",
       " ['wayne', 102],\n",
       " ['control', 102],\n",
       " ['btw', 102],\n",
       " ['walk', 101],\n",
       " ['daily', 101],\n",
       " ['hoes', 101],\n",
       " ['blocked', 101],\n",
       " ['earth', 101],\n",
       " ['pass', 101],\n",
       " ['constantly', 101],\n",
       " ['huh', 101],\n",
       " ['bus', 100],\n",
       " ['trust', 100],\n",
       " ['car', 100],\n",
       " ['terror', 100],\n",
       " ['rules', 100],\n",
       " ['dear', 100],\n",
       " ['hating', 100],\n",
       " ['lmfao', 100],\n",
       " ['acts', 100],\n",
       " ['taught', 100],\n",
       " ['lady', 100],\n",
       " ['israeliregime', 99],\n",
       " ['break', 99],\n",
       " ['sharia', 99],\n",
       " ['truly', 99],\n",
       " ['hands', 99],\n",
       " ['uk', 99],\n",
       " ['uh', 98],\n",
       " ['london', 98],\n",
       " ['outta', 98],\n",
       " ['drive', 98],\n",
       " ['created', 98],\n",
       " ['forced', 98],\n",
       " ['common', 98],\n",
       " ['office', 98],\n",
       " ['assault', 98],\n",
       " ['hi', 97],\n",
       " ['lgbt', 97],\n",
       " ['dogs', 97],\n",
       " ['played', 97],\n",
       " ['normal', 97],\n",
       " ['vs', 97],\n",
       " ['meant', 97],\n",
       " ['chinese', 97],\n",
       " ['moment', 97],\n",
       " ['huge', 97],\n",
       " ['beautiful', 97],\n",
       " ['pray', 97],\n",
       " ['song', 97],\n",
       " ['share', 97],\n",
       " ['minorities', 97],\n",
       " ['worry', 96],\n",
       " ['create', 96],\n",
       " ['cover', 96],\n",
       " ['watched', 95],\n",
       " ['page', 95],\n",
       " ['couple', 95],\n",
       " ['private', 95],\n",
       " ['genocide', 95],\n",
       " ['learned', 95],\n",
       " ['colin', 94],\n",
       " ['perfect', 94],\n",
       " ['marriage', 94],\n",
       " ['along', 94],\n",
       " ['rich', 94],\n",
       " ['future', 94],\n",
       " ['anyway', 94],\n",
       " ['weight', 94],\n",
       " ['faggot', 94],\n",
       " ['pakistani', 94],\n",
       " ['cyber', 93],\n",
       " ['c', 93],\n",
       " ['awesome', 93],\n",
       " ['order', 93],\n",
       " ['justice', 93],\n",
       " ['main', 93],\n",
       " ['grew', 93],\n",
       " ['yea', 93],\n",
       " ['peoples', 93],\n",
       " ['christianity', 93],\n",
       " ['realdonaldtrump', 93],\n",
       " ['actions', 92],\n",
       " ['racists', 92],\n",
       " ['language', 92],\n",
       " ['hilarious', 92],\n",
       " ['posting', 92],\n",
       " ['theyve', 92],\n",
       " ['enjoy', 92],\n",
       " ['brain', 92],\n",
       " ['somebody', 92],\n",
       " ['agenda', 92],\n",
       " ['population', 92],\n",
       " ['retarded', 92],\n",
       " ['protect', 92],\n",
       " ['slut', 92],\n",
       " ['congress', 92],\n",
       " ['sign', 91],\n",
       " ['sending', 91],\n",
       " ['forever', 91],\n",
       " ['mention', 91],\n",
       " ['lose', 91],\n",
       " ['allow', 90],\n",
       " ['till', 90],\n",
       " ['videos', 90],\n",
       " ['english', 90],\n",
       " ['beyond', 90],\n",
       " ['grown', 90],\n",
       " ['wouldve', 90],\n",
       " ['forgot', 90],\n",
       " ['treat', 90],\n",
       " ['meaning', 90],\n",
       " ['f', 90],\n",
       " ['starting', 90],\n",
       " ['treated', 90],\n",
       " ['talked', 90],\n",
       " ['negros', 90],\n",
       " ['join', 89],\n",
       " ['month', 89],\n",
       " ['crush', 89],\n",
       " ['ended', 89],\n",
       " ['multiple', 89],\n",
       " ['bye', 89],\n",
       " ['fair', 89],\n",
       " ['paid', 89],\n",
       " ['liberals', 89],\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(words_count, key = lambda x : x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772264b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3693349c",
   "metadata": {},
   "source": [
    "### Using a pretrained GloVe embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78a57e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "757821eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de95d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load(f'glove-twitter-{embedd_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "185a3dec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clean_tok'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3w/t9rjzh2178b75nwtqt79kkzh0000gn/T/ipykernel_55738/3000580442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglove_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"not in voc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedd_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_tok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5581\u001b[0m         ):\n\u001b[1;32m   5582\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clean_tok'"
     ]
    }
   ],
   "source": [
    "glove_vectors.add_vector(key = \"not in voc\", vector = np.zeros(embedd_len))\n",
    "max_length = df.clean_tok_tweet.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4bf1f1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feminazi',\n",
       " 'actual',\n",
       " 'word',\n",
       " 'denotnasharchy',\n",
       " 'job',\n",
       " 'mean',\n",
       " 'protect',\n",
       " 'people',\n",
       " 'even',\n",
       " 'people',\n",
       " 'dont',\n",
       " 'agree',\n",
       " 'withlikethey',\n",
       " 'barely',\n",
       " 'cook',\n",
       " 'anything',\n",
       " 'entre',\n",
       " 'mkrrt',\n",
       " 'kf',\n",
       " 'like',\n",
       " 'community',\n",
       " 'foster',\n",
       " 'jerk',\n",
       " 'prevent',\n",
       " 'user',\n",
       " 'adopt',\n",
       " 'communitys',\n",
       " 'code',\n",
       " 'much',\n",
       " 'technical',\n",
       " 'problemits',\n",
       " 'almost',\n",
       " 'time',\n",
       " 'jamesgweenwood',\n",
       " 'shock',\n",
       " 'youre',\n",
       " 'bully',\n",
       " 'againlionlioneateat',\n",
       " 'yeah',\n",
       " 'someone',\n",
       " 'dmed',\n",
       " 'screenshot',\n",
       " 'meh',\n",
       " 'let',\n",
       " 'idea',\n",
       " 'even',\n",
       " 'isfor',\n",
       " 'egregious',\n",
       " 'case',\n",
       " 'harassment',\n",
       " 'definitely',\n",
       " 'need',\n",
       " 'able',\n",
       " 'respond',\n",
       " 'appropriately',\n",
       " 'toxicity',\n",
       " 'isnt',\n",
       " 'thisso',\n",
       " 'happen',\n",
       " 'httptcozptrtsyfivnibelsnarfabarf',\n",
       " 'srhbutts',\n",
       " 'grimachu',\n",
       " 'really',\n",
       " 'funny',\n",
       " 'assumption',\n",
       " 'make',\n",
       " 'work',\n",
       " 'much',\n",
       " 'wrongthe',\n",
       " 'lack',\n",
       " 'selfawareness',\n",
       " 'wadhwa',\n",
       " 'right',\n",
       " 'stagger',\n",
       " 'hilarious',\n",
       " 'stopwadhwahahahaha',\n",
       " 'httptcojrpkjcnvhthis',\n",
       " 'go',\n",
       " 'well',\n",
       " 'finale',\n",
       " 'meet',\n",
       " 'mother',\n",
       " 'one',\n",
       " 'way',\n",
       " 'anothertrump',\n",
       " 'evil',\n",
       " 'cabal',\n",
       " 'criminal',\n",
       " 'catch',\n",
       " 'lie',\n",
       " 'as',\n",
       " 'impeach',\n",
       " 'wont',\n",
       " 'b',\n",
       " 'moment',\n",
       " 'soonsaintneko',\n",
       " 'love',\n",
       " 'rock',\n",
       " 'band',\n",
       " 'pi',\n",
       " 'dedicate',\n",
       " 'thread',\n",
       " 'swiftonsecurityrt',\n",
       " 'srhbutts',\n",
       " 'woman',\n",
       " 'gdc',\n",
       " 'share',\n",
       " 'heartbreaking',\n",
       " 'story',\n",
       " 'gamergate',\n",
       " 'chooses',\n",
       " 'respond',\n",
       " 'httptcoqabwvqnslainvfr',\n",
       " 'try',\n",
       " 'still',\n",
       " 'make',\n",
       " 'mistake',\n",
       " 'hardif',\n",
       " 'think',\n",
       " 'president',\n",
       " 'need',\n",
       " 'congressional',\n",
       " 'approval',\n",
       " 'idiot',\n",
       " 'rt',\n",
       " 'empresssudol',\n",
       " 'freebsdgirl',\n",
       " 'thats',\n",
       " 'though',\n",
       " 'even',\n",
       " 'book',\n",
       " 'reject',\n",
       " 'first',\n",
       " 'draft',\n",
       " 'kill',\n",
       " 'mockingbird',\n",
       " 'beforaint',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'lie',\n",
       " 'think',\n",
       " 'might',\n",
       " 'try',\n",
       " 'pomegranate',\n",
       " 'cous',\n",
       " 'cous',\n",
       " 'salad',\n",
       " 'sound',\n",
       " 'yum',\n",
       " 'mkrgtfeminazibrittanyblade',\n",
       " 'feminazifront',\n",
       " 'well',\n",
       " 'arent',\n",
       " 'isi',\n",
       " 'claim',\n",
       " 'muslim',\n",
       " 'theyre',\n",
       " 'belief',\n",
       " 'nothing',\n",
       " 'like',\n",
       " 'dont',\n",
       " 'harpcast',\n",
       " 'zajice',\n",
       " 'dont',\n",
       " 'fuck',\n",
       " 'ever',\n",
       " 'know',\n",
       " 'anything',\n",
       " 'go',\n",
       " 'onlarge',\n",
       " 'spidersconlanwebster',\n",
       " 'pretty',\n",
       " 'rad',\n",
       " 'nonstandard',\n",
       " 'name',\n",
       " 'although',\n",
       " 'there',\n",
       " 'get',\n",
       " 'randi',\n",
       " 'harper',\n",
       " 'world',\n",
       " 'poor',\n",
       " 'thingspeople',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'generosity',\n",
       " 'worst',\n",
       " 'never',\n",
       " 'offer',\n",
       " 'cheetohs',\n",
       " 'greedy',\n",
       " 'bastardsi',\n",
       " 'hate',\n",
       " 'person',\n",
       " 'assume',\n",
       " 'shyt',\n",
       " 'like',\n",
       " 'come',\n",
       " 'fuck',\n",
       " 'ask',\n",
       " 'clarify',\n",
       " 'aint',\n",
       " 'go',\n",
       " 'lie',\n",
       " 'dumbass',\n",
       " 'rt',\n",
       " 'missclarolyn',\n",
       " 'didnt',\n",
       " 'give',\n",
       " 'deconstruct',\n",
       " 'lemon',\n",
       " 'tart',\n",
       " 'anyway',\n",
       " 'item',\n",
       " 'wasnt',\n",
       " 'menu',\n",
       " 'mkrso',\n",
       " 'selena',\n",
       " 'hate',\n",
       " 'account',\n",
       " 'think',\n",
       " 'nolan',\n",
       " 'cyber',\n",
       " 'bully',\n",
       " 'ya',\n",
       " 'tht',\n",
       " 'make',\n",
       " 'lot',\n",
       " 'sense',\n",
       " 'nottitanfall',\n",
       " 'saltball',\n",
       " 'grapple',\n",
       " 'ronin',\n",
       " 'music',\n",
       " 'iniquity',\n",
       " 'httpstcoxblxunqjx',\n",
       " 'via',\n",
       " 'youtube',\n",
       " 'shit',\n",
       " 'sick',\n",
       " 'dude',\n",
       " 'iniqutycharlottesph',\n",
       " 'even',\n",
       " 'though',\n",
       " 'girl',\n",
       " 'bully',\n",
       " 'zachs',\n",
       " 'mom',\n",
       " 'kind',\n",
       " 'rude',\n",
       " 'know',\n",
       " 'good',\n",
       " 'say',\n",
       " 'thatkatelinreed',\n",
       " 'big',\n",
       " 'ole',\n",
       " 'mean',\n",
       " 'bully',\n",
       " 'sister',\n",
       " 'dicetechjobs',\n",
       " 'itshelladom',\n",
       " 'mairabenjamin',\n",
       " 'jenrpetersen',\n",
       " 'isnt',\n",
       " 'mert',\n",
       " 'squldz',\n",
       " 'bitch',\n",
       " 'wan',\n",
       " 'na',\n",
       " 'penguin',\n",
       " 'bad',\n",
       " 'httpstcopkynmxcbsi',\n",
       " 'get',\n",
       " 'really',\n",
       " 'read',\n",
       " 'blue',\n",
       " 'apron',\n",
       " 'recipe',\n",
       " 'realize',\n",
       " 'dont',\n",
       " 'use',\n",
       " 'oxford',\n",
       " 'comma',\n",
       " 'httpstcovnbwqnvindsayymiaa',\n",
       " 'lmao',\n",
       " 'naw',\n",
       " 'nobody',\n",
       " 'like',\n",
       " 'thank',\n",
       " 'youre',\n",
       " 'nice',\n",
       " 'youre',\n",
       " 'try',\n",
       " 'bully',\n",
       " 'pplmcmahoniel',\n",
       " 'there',\n",
       " 'lot',\n",
       " 'sarcasm',\n",
       " 'doublespeak',\n",
       " 'group',\n",
       " 'especially',\n",
       " 'appropriation',\n",
       " 'term',\n",
       " 'text',\n",
       " 'filter',\n",
       " 'dont',\n",
       " 'workrt',\n",
       " 'atonal',\n",
       " 'freebsdgirl',\n",
       " 'comment',\n",
       " 'article',\n",
       " 'remind',\n",
       " 'stilget',\n",
       " 'do',\n",
       " 'mansplainers',\n",
       " 'httptcophzepvncx',\n",
       " 'httpnintendo',\n",
       " 'd',\n",
       " 'httptcohbxvcuvvnkmcmahoniel',\n",
       " 'unfortunately',\n",
       " 'text',\n",
       " 'filter',\n",
       " 'highriskcathmayoevans',\n",
       " 'julesrisk',\n",
       " 'u',\n",
       " 'virtual',\n",
       " 'bully',\n",
       " 'make',\n",
       " 'hit',\n",
       " 'loltthacker',\n",
       " 'frazercobb',\n",
       " 'two',\n",
       " 'stop',\n",
       " 'bullying',\n",
       " 'thankyouwavinator',\n",
       " 'chobitcoin',\n",
       " 'love',\n",
       " 'hts',\n",
       " 'like',\n",
       " 'thisi',\n",
       " 'spin',\n",
       " 'gallery',\n",
       " 'tablet',\n",
       " 'like',\n",
       " 'price',\n",
       " 'right',\n",
       " 'wheel',\n",
       " 'use',\n",
       " 'whatever',\n",
       " 'come',\n",
       " 'upmattcooke',\n",
       " 'nutrition',\n",
       " 'key',\n",
       " 'close',\n",
       " 'aboriginal',\n",
       " 'life',\n",
       " 'expectancy',\n",
       " 'gap',\n",
       " 'httptcoihtnirjma',\n",
       " 'croakeyblog',\n",
       " 'httptcoktnqadtmcheck',\n",
       " 'guest',\n",
       " 'post',\n",
       " 'zusterschap',\n",
       " 'blog',\n",
       " 'httptcowkgrwsbov',\n",
       " 'response',\n",
       " 'blameonenotall',\n",
       " 'threaddarxtorm',\n",
       " 'sighfinally',\n",
       " 'meal',\n",
       " 'without',\n",
       " 'dracko',\n",
       " 'piping',\n",
       " 'mkrplumsofdoom',\n",
       " 'unfairly',\n",
       " 'antagonize',\n",
       " 'bully',\n",
       " 'barton',\n",
       " 'want',\n",
       " 'team',\n",
       " 'bastad',\n",
       " 'one',\n",
       " 'lolhttpstcolkurhkvfrt',\n",
       " 'amaninblack',\n",
       " 'freebsdgirl',\n",
       " 'might',\n",
       " 'want',\n",
       " 'take',\n",
       " 'whoever',\n",
       " 'person',\n",
       " 'twitter',\n",
       " 'httpstcoqsudicriephow',\n",
       " 'even',\n",
       " 'classify',\n",
       " 'wield',\n",
       " 'mob',\n",
       " 'indirect',\n",
       " 'bully',\n",
       " 'im',\n",
       " 'surewhy',\n",
       " 'kelly',\n",
       " 'attack',\n",
       " 'brie',\n",
       " 'first',\n",
       " 'attack',\n",
       " 'behind',\n",
       " 'attack',\n",
       " 'unprovoked',\n",
       " 'shes',\n",
       " 'big',\n",
       " 'bully',\n",
       " 'divas',\n",
       " 'destructionleyna',\n",
       " 'case',\n",
       " 'evidently',\n",
       " 'yesstibbons',\n",
       " 'use',\n",
       " 'prt',\n",
       " 'slavenout',\n",
       " 'bilic',\n",
       " 'fuck',\n",
       " 'hate',\n",
       " 'watch',\n",
       " 'u',\n",
       " 'lose',\n",
       " 'swansea',\n",
       " 'relegation',\n",
       " 'imminentmdmpsy',\n",
       " 'debt',\n",
       " 'youre',\n",
       " 'fuck',\n",
       " 'hole',\n",
       " 'pathetic',\n",
       " 'as',\n",
       " 'boyfriend',\n",
       " 'put',\n",
       " 'httpstcogdwpqgcui']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_tok_tweet[df.clean_tok_tweet.apply(len) == max_length].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93ee27d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.clean_tok_tweet.apply(len) > 40].cyberbullying_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0367887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(sentence, length, pad = 'not in voc'):\n",
    "    diff_lengths = length - len(sentence)\n",
    "    if diff_lengths > 0:\n",
    "        return sentence + [pad for i in range(diff_lengths)]\n",
    "    elif diff_lengths == 0:\n",
    "        return sentence\n",
    "    else:\n",
    "        raise Exception(\"Not appropriate max final length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5f76ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences):\n",
    "    words_registered = []\n",
    "    words_index = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        for w in sentence:\n",
    "            if w not in words_registered:\n",
    "                words_registered.append(w)\n",
    "                words_index.append(i)\n",
    "                i+=1\n",
    "            else:\n",
    "                continue\n",
    "    return dict(zip(words_registered, words_index))\n",
    "\n",
    "vocabulary = create_vocabulary(df.clean_tok_tweet)\n",
    "vocabulary[\"not in voc\"] = len(vocabulary)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38926e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((vocab_size, embedd_len))\n",
    "for key in vocabulary.keys():\n",
    "    try:\n",
    "        embed_matrix[vocabulary[key]] = glove_vectors[key]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22d92199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanEmbedSentence(sentence, KeyedVector, d_embed = 50):\n",
    "    if len(sentence) > 0:\n",
    "        vectors = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vectors.append(KeyedVector[word])\n",
    "            except:\n",
    "                vectors.append(np.zeros(d_embed))\n",
    "        return np.array(vectors).mean(axis = 0)\n",
    "    else:\n",
    "        return np.zeros(d_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11d64873",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gv = df.clean_tok_tweet.apply(lambda x : MeanEmbedSentence(x, glove_vectors, embedd_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6414756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gv = np.array([x for x in x_gv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "632b1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gv, X_test_gv, y_train_gv, y_test_gv = train_test_split(X_gv,\n",
    "                                                    df[\"cyberbullying_type\"],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state = 0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0e112d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61393234,  0.09909167, -0.27856333, ...,  0.34815332,\n",
       "         0.0096385 ,  0.38089   ],\n",
       "       [ 0.12575366,  0.23154466, -0.02757628, ...,  0.17319603,\n",
       "        -0.12567182,  0.05321825],\n",
       "       [ 0.74423122,  0.36952874, -0.19222251, ...,  0.43436208,\n",
       "         0.08791625,  0.3450079 ],\n",
       "       ...,\n",
       "       [ 0.44971201,  0.38231599, -0.1024151 , ...,  0.39961329,\n",
       "         0.1039636 ,  0.101904  ],\n",
       "       [ 0.27169004,  0.11556547, -0.08892915, ...,  0.12240506,\n",
       "        -0.00957426,  0.07988633],\n",
       "       [ 0.08403   ,  0.52644998, -0.21923   , ..., -0.142985  ,\n",
       "         0.153145  ,  0.08888   ]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf88fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gv = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
    "rf_gv.fit(X_train_gv, y_train_gv)\n",
    "y_pred_gv = rf_gv.predict(X_test_gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac90dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7130975677942409\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred_gv, y_test_gv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595aad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
